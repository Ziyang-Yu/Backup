\begin{section}{简介}
    
图神经网络在分析非欧几里得图数据方面取得了令人印象深刻的成功，并在各种应用中取得了可喜的成果，包括社交网络、推荐系统和知识图等~\cite{dai2016discriminative,ying2018graph,lei2019gcn}。尽管GNN有很大的前景，但在应用于现实世界中常见的大型图时，GNN遇到了巨大的挑战--大型图的节点数量可以达到数百万甚至数十亿。
在大型图上训练GNN，由于缺乏内在的并行性，共同面临着挑战。
在大型图上训练GNN，由于反向传播优化中缺乏固有的并行性，以及图节点之间严重的相互依赖性，共同构成了挑战。
为了应对这种独特的挑战，分布式GNN训练是一个很有前景的开放领域，近年来吸引了越来越多的关注~\cite{dorylus_osdi21,ramezani2021learn,wan2022pipegcn,chai2022distributed}，并已成为在大型图上快速准确训练的标准。



现有的分布式GNN训练方法可以分为两类，即"\emph{基于分割}"和"\emph{基于传播}"，根据它们如何解决\emph{计算/通信成本}和\emph{信息损失}之间的权衡。"基于分割"方法~\cite{angerd2020distributed,jia2020improving,ramezani2021learn}通过丢弃跨子图的边，将图分割成不同的子图，因此大图上的GNN训练被分解成多个较小的训练任务，每个训练任务在一个孤立的子图中并行，减少子图间的通信。
然而，这将导致严重的信息损失，因为对子图之间节点的依赖关系一无所知，并导致性能下降。
为了减轻信息损失，"基于传播 "的方法~\cite{ma2019neugraph,zhu2019aligraph,zheng2020distdgl,tripathy2020reducing}不忽略不同子图之间的边，子图之间的邻居通信，以满足GNN的邻居聚合。然而，随着GNN的深入，参与邻居聚合的邻居数量呈指数级增长（即， \emph{邻居爆炸}~\cite{hamilton2017inductive}），因此不可避免地遭受巨大的通信开销和困扰的效率。
打破基于分区和传播的方法的悖论的一个自然方法是通过增加第三个维度，形成一个 \emph{抵消三角}。具体来说，通过利用离线存储器来存储子图外邻居的 \emph{历史} 嵌入，并在下一个阶段需要时将其拉到GPU上，可以实现相对于节点数量的恒定通信成本，同时保留子图之间的相互依赖关系。这种技术~\cite{chen2018stochastic,fey2021gnnautoscale,wan2022pipegcn,chai2022distributed}已被广泛用于分布式GNN训练，并取得了最先进的性能。
\end{section}