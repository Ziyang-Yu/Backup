% !Mode:: "TeX:UTF-8"
% !TEX program  = xelatex
\begin{中文摘要}{\LaTeX ；接口}
  尽管最近GNN取得了成功，但在具有数百万节点和数十亿条边的大型图上训练GNN仍然具有挑战性，这在许多现实世界的应用中是普遍存在的。在分布式GNN训练中，"基于分区 "的方法享有较低的通信成本，但由于掉落的边而遭受信息损失，而 "基于传播 "的方法避免了信息损失，但由于邻居爆炸而遭受令人望而却步的通信开销。这两者之间的一个自然的 "中间点 "是通过缓存历史信息（例如，节点嵌入），这可以实现恒定的通信成本，同时保留不同分区之间的相互依赖。然而，这样的好处是以涉及过时的训练信息为代价的，从而导致呆板性、不精确性和收敛问题。为了克服这些挑战，本文提出了SAT（滞后性缓解训练），这是一个新颖的、可扩展的分布式GNN训练框架，可以自适应地减少嵌入的滞后性。具体来说，我们提出将嵌入预测建模为学习由历史嵌入引起的时间图，并建立一个子图不变的弱监督辅助模块来预测未来的嵌入，该模块以数据驱动的方式减少呆滞性，并具有良好的可扩展性。预测的嵌入作为一个桥梁，以在线的方式交替训练分布式GNN和辅助模块。最后，我们提供了广泛的理论和经验分析
  来证明所提出的方法可以在性能和收敛速度上大大改善现有的框架，而只需最小的额外费用。
  
\end{中文摘要}

\begin{英文摘要}{LaTeX, Interface}
  \lipsum[1]
\end{英文摘要}
